{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596c7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "from utils.dataset_helper import create_dataloaders\n",
    "from utils.image_utils import render\n",
    "from utils.diffusion_data_helper import denormalize_data\n",
    "from utils.gaussian_file_helper import load_gaussians\n",
    "from vae_model import GaussianVAE\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config_path = \"config/vae_training.yaml\"\n",
    "checkpoint_path = \"best_gaussian_vae.pth\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba1eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22084 files in ./data/FFHQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianVAE(\n",
       "  (sa1): PointNetSetAbstractionMsg(\n",
       "    (conv_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (bn_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-1): 2 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sa2): PointNetSetAbstractionMsg(\n",
       "    (conv_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Conv2d(322, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1-2): 2 x ModuleList(\n",
       "        (0): Conv2d(322, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (bn_blocks): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1-2): 2 x ModuleList(\n",
       "        (0-1): 2 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sa3): PointNetSetAbstraction(\n",
       "    (mlp_convs): ModuleList(\n",
       "      (0): Conv2d(642, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (mlp_bns): ModuleList(\n",
       "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc_logvar): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (decoder): GaussianTransformerDecoder(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head_xy): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (head_scale): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (head_rot): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (head_color): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianVAE(\n",
    "            num_gaussians=cfg[\"model\"][\"num_gaussians\"],\n",
    "            input_dim=cfg[\"model\"][\"input_dim\"],\n",
    "            latent_dim=cfg[\"model\"][\"model_dim\"],\n",
    "            decoder_layers=cfg[\"model\"].get(\"decoder_transformer_layers\", 6),\n",
    "            decoder_heads=cfg[\"model\"].get(\"decoder_transformer_heads\", 8)\n",
    "        ).to(device)\n",
    "\n",
    "data_loader, _ = create_dataloaders(\n",
    "    \"./data/FFHQ\",\n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    augment=False,\n",
    "    is_distributed=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd1c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 150 frames for 5s video at 30 fps...\n"
     ]
    }
   ],
   "source": [
    "# Video parameters\n",
    "fps = 30\n",
    "duration = 5  # seconds\n",
    "num_frames = fps * duration\n",
    "\n",
    "# Output path\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "video_path = os.path.join(output_dir, \"latent_interpolation.mp4\")\n",
    "\n",
    "print(f\"Generating {num_frames} frames for {duration}s video at {fps} fps...\")\n",
    "\n",
    "# Get two random samples from the dataset\n",
    "data_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = next(data_iter).to(device)\n",
    "data2 = next(data_iter).to(device)\n",
    "\n",
    "# Encode to get latent means\n",
    "with torch.no_grad():\n",
    "    mu1, logvar1 = model.encode(data1)\n",
    "    mu2, logvar2 = model.encode(data2)\n",
    "\n",
    "# Create video writer (requires imageio-ffmpeg backend)\n",
    "with imageio.get_writer(video_path, fps=fps, format=\"FFMPEG\", codec=\"libx264\", quality=8) as writer:\n",
    "    for i in tqdm(range(num_frames)):\n",
    "        # Calculate alpha (0 to 1)\n",
    "        alpha = i / (num_frames - 1)\n",
    "        \n",
    "        # Interpolate in latent space\n",
    "        latent_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "        \n",
    "        # Decode and render\n",
    "        with torch.no_grad():\n",
    "            decoded = model.decode(latent_interp)\n",
    "        \n",
    "        xy, scale, rot, feat = denormalize_data(decoded[:, :, 0:2], decoded[:, :, 2:4], \n",
    "                                                decoded[:, :, 4:5], decoded[:, :, 5:8])\n",
    "        \n",
    "        xy = xy.squeeze(0).contiguous().float()\n",
    "        scale = scale.squeeze(0).contiguous().float()\n",
    "        rot = rot.squeeze(0).contiguous().float()\n",
    "        feat = feat.squeeze(0).contiguous().float()\n",
    "        \n",
    "        img_size = (int(480), int(640))\n",
    "        image = render(xy, scale, rot, feat, img_size=img_size)\n",
    "        image_np = image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Convert to uint8 (0-255 range)\n",
    "        image_uint8 = (np.clip(image_np, 0, 1) * 255).astype(np.uint8)\n",
    "        writer.append_data(image_uint8)\n",
    "\n",
    "print(f\"Video saved to {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b99ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also save an interpolation as a series of images for reference, using the 2 extremes 3 frames in between\n",
    "image_output_dir = os.path.join(output_dir, \"interpolation_frames\")\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "num_image_frames = 5\n",
    "for i in range(num_image_frames):\n",
    "    alpha = i / (num_image_frames - 1)\n",
    "    \n",
    "    latent_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        decoded = model.decode(latent_interp)\n",
    "    \n",
    "    xy, scale, rot, feat = denormalize_data(decoded[:, :, 0:2], decoded[:, :, 2:4], \n",
    "                                            decoded[:, :, 4:5], decoded[:, :, 5:8])\n",
    "    \n",
    "    xy = xy.squeeze(0).contiguous().float()\n",
    "    scale = scale.squeeze(0).contiguous().float()\n",
    "    rot = rot.squeeze(0).contiguous().float()\n",
    "    feat = feat.squeeze(0).contiguous().float()\n",
    "    \n",
    "    img_size = (int(480), int(640))\n",
    "    image = render(xy, scale, rot, feat, img_size=img_size)\n",
    "    image_np = image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    image_uint8 = (np.clip(image_np, 0, 1) * 255).astype(np.uint8)\n",
    "    imageio.imwrite(os.path.join(image_output_dir, f\"frame_{i:02d}.png\"), image_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0080c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to do a giant render of a single sample, it should be at least 10k by 10k pixels\n",
    "data_sample = next(data_iter).to(device)\n",
    "with torch.no_grad():\n",
    "    mu, logvar = model.encode(data_sample)\n",
    "    decoded = model.decode(mu)\n",
    "\n",
    "    xy, scale, rot, feat = denormalize_data(decoded[:, :, 0:2], decoded[:, :, 2:4],\n",
    "                                            decoded[:, :, 4:5], decoded[:, :, 5:8])\n",
    "    \n",
    "    xy = xy.squeeze(0).contiguous().float()\n",
    "    scale = scale.squeeze(0).contiguous().float()\n",
    "    rot = rot.squeeze(0).contiguous().float()\n",
    "    feat = feat.squeeze(0).contiguous().float()\n",
    "\n",
    "    img_size = (4800, 6400)\n",
    "    large_image = render(xy, scale * 10.0, rot, feat, img_size=img_size)\n",
    "    large_image_np = large_image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    large_image_uint8 = (np.clip(large_image_np, 0, 1) * 255).astype(np.uint8)\n",
    "    large_image_path = os.path.join(output_dir, \"large_render.png\")\n",
    "    imageio.imwrite(large_image_path, large_image_uint8)\n",
    "\n",
    "    img_size = (480, 640)\n",
    "    small_image = render(xy, scale, rot, feat, img_size=img_size)\n",
    "    small_image_np = small_image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    small_image_uint8 = (np.clip(small_image_np, 0, 1) * 255).astype(np.uint8)\n",
    "    small_image_path = os.path.join(output_dir, \"small_render.png\")\n",
    "    imageio.imwrite(small_image_path, small_image_uint8)\n",
    "\n",
    "    #Let's print out which inputs we used\n",
    "    print(\"Used the following two samples for interpolation:\")\n",
    "    print(data_sample)\n",
    "    print(f\"Small render saved to {small_image_path}\")\n",
    "    print(f\"Giant render saved to {large_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc42489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering 22084 files to output/all_renders...\n",
      "00000.png.npz\n"
     ]
    }
   ],
   "source": [
    "#Let's render every single npz in the data folder and save the images to a folder\n",
    "render_output_dir = os.path.join(output_dir, \"all_renders\")\n",
    "os.makedirs(render_output_dir, exist_ok=True)\n",
    "data_filenames = sorted([f for f in os.listdir(\"./data/FFHQ\") if f.endswith(\".npz\")])\n",
    "print(f\"Rendering {len(data_filenames)} files to {render_output_dir}...\")\n",
    "print(data_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73432e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22084/22084 [04:27<00:00, 82.48it/s]   \n"
     ]
    }
   ],
   "source": [
    "for filename in tqdm(data_filenames):\n",
    "    data_path = os.path.join(\"./data/FFHQ\", filename)\n",
    "    if os.path.exists(os.path.join(render_output_dir, \"gt\",f\"{os.path.splitext(filename)[0]}_render_gt.png\")):\n",
    "        continue  #Skip already rendered files\n",
    "\n",
    "    data =  load_gaussians(data_path)\n",
    "    xy = data['xy'].unsqueeze(0).to(device)\n",
    "    scale = data['scale'].unsqueeze(0).to(device) \n",
    "    rot = data['rot'].unsqueeze(0).to(device)\n",
    "    feat = data['feat'].unsqueeze(0).to(device)\n",
    "    input_data = torch.cat([xy, scale, rot, feat], dim=2)\n",
    "\n",
    "    #Render ground truth\n",
    "    image_gt = render(xy.squeeze(0), scale.squeeze(0)/2, rot.squeeze(0), feat.squeeze(0), img_size=(256, 256))\n",
    "    image_gt_np = image_gt.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    image_gt_uint8 = (np.clip(image_gt_np, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    output_image_path_gt = os.path.join(render_output_dir, \"gt\",f\"{os.path.splitext(filename)[0]}_render_gt.png\")\n",
    "    imageio.imwrite(output_image_path_gt, image_gt_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf16e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Render VAE reconstruction\n",
    "from utils.dataset_helper import GaussianSplatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create dataset ensuring order matches data_filenames\n",
    "file_paths = [os.path.join(\"./data/FFHQ\", f) for f in data_filenames]\n",
    "dataset = GaussianSplatDataset(\"./data/FFHQ\", file_paths=file_paths, augment=False)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for (filename, input_data) in tqdm(zip(data_filenames, loader), total=len(data_filenames)):\n",
    "    \n",
    "    if os.path.exists(os.path.join(render_output_dir, \"vae\",f\"{os.path.splitext(filename)[0]}_render_vae.png\")):\n",
    "        continue  #Skip already rendered files\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_data = input_data.to(device)\n",
    "        mu, logvar = model.encode(input_data)\n",
    "        decoded = model.decode(mu)\n",
    "\n",
    "        xy_dec, scale_dec, rot_dec, feat_dec = denormalize_data(decoded[:, :, 0:2], decoded[:, :, 2:4],\n",
    "                                                decoded[:, :, 4:5], decoded[:, :, 5:8])\n",
    "        \n",
    "        xy_dec = xy_dec.squeeze(0).contiguous().float()\n",
    "        scale_dec = scale_dec.squeeze(0).contiguous().float()\n",
    "        rot_dec = rot_dec.squeeze(0).contiguous().float()\n",
    "        feat_dec = feat_dec.squeeze(0).contiguous().float()\n",
    "\n",
    "        image_vae = render(xy_dec, scale_dec/2, rot_dec, feat_dec, img_size=(256, 256))\n",
    "        image_vae_np = image_vae.cpu().detach().permute(1, 2, 0).numpy()\n",
    "        image_vae_uint8 = (np.clip(image_vae_np, 0, 1) * 255).astype(np.uint8)\n",
    "        \n",
    "        output_image_path_vae = os.path.join(render_output_dir, \"vae\", f\"{os.path.splitext(filename)[0]}_render_vae.png\")\n",
    "        imageio.imwrite(output_image_path_vae, image_vae_uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a0fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown file in render output dir: gt_renders\n",
      "Unknown file in render output dir: vae_renders\n",
      "Calculating FID and KID between 0 GT renders and 0 VAE renders...\n",
      "Computing FID and KID, this may take a while...\n",
      "compute FID between two folders\n",
      "Found 22083 images in the folder output/all_renders/gt_renders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID gt_renders : 100%|██████████| 691/691 [00:54<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22084 images in the folder output/all_renders/vae_renders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID vae_renders : 100%|██████████| 691/691 [00:54<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID computation took 118.58 seconds.\n",
      "FID: 95.78872547084262\n",
      "compute KID between two folders\n",
      "Found 22083 images in the folder output/all_renders/gt_renders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID gt_renders : 100%|██████████| 691/691 [00:55<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22084 images in the folder output/all_renders/vae_renders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KID vae_renders : 100%|██████████| 691/691 [00:54<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KID computation took 118.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate the FID and KID between the GT renders and the VAE renders\n",
    "import os\n",
    "import time\n",
    "# First we split the images into two folders\n",
    "gt = 0\n",
    "os.makedirs(os.path.join(render_output_dir, \"gt_renders\"), exist_ok=True)\n",
    "vae = 0\n",
    "os.makedirs(os.path.join(render_output_dir, \"vae_renders\"), exist_ok=True)\n",
    "\n",
    "\n",
    "for i in os.listdir(render_output_dir):\n",
    "    if i.endswith(\"_render_gt.png\"):\n",
    "        os.rename(os.path.join(render_output_dir, i), os.path.join(render_output_dir, \"gt_renders\", i))\n",
    "        gt += 1\n",
    "    elif i.endswith(\"_render_vae.png\"):\n",
    "        os.rename(os.path.join(render_output_dir, i), os.path.join(render_output_dir, \"vae_renders\", i))\n",
    "        vae += 1\n",
    "    else:\n",
    "        print(f\"Unknown file in render output dir: {i}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Calculating FID and KID between {gt} GT renders and {vae} VAE renders...\")\n",
    "\n",
    "from cleanfid import fid\n",
    "print(\"Computing FID and KID, this may take a while...\")\n",
    "start_time =  time.time()\n",
    "fid_value = fid.compute_fid(os.path.join(render_output_dir, \"gt_renders\"),\n",
    "                            os.path.join(render_output_dir, \"vae_renders\"),\n",
    "                            mode=\"clean\",\n",
    "                            num_workers=16)\n",
    "end_time = time.time()\n",
    "print(f\"FID computation took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"FID: {fid_value}\")\n",
    "\n",
    "start_time =  time.time()\n",
    "kid_value = fid.compute_kid(os.path.join(render_output_dir, \"gt_renders\"),\n",
    "                            os.path.join(render_output_dir, \"vae_renders\"),\n",
    "                            mode=\"clean\",\n",
    "                            num_workers=16)\n",
    "end_time = time.time()\n",
    "print(f\"KID computation took {end_time - start_time:.2f} seconds.\")\n",
    "print(f\"KID: {kid_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cec3d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KID: 0.09401272982358932\n"
     ]
    }
   ],
   "source": [
    "print(f\"KID: {kid_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
