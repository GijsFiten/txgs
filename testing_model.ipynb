{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26925dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import GaussianDiffusionTransformer\n",
    "from utils.image_utils import render_and_save\n",
    "from utils.diffusion_data_helper import denormalize_data, DiffusionScheduler\n",
    "from utils.dataset_helper import create_dataloaders\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481eab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"data_dir\": \"./data/small/\",\n",
    "    \"output_dir\": \"./output/\",\n",
    "    \"batch_size\": 32,            # Small physical batch size\n",
    "    \"grad_accumulation\": 4,     # Effective batch size = 192 (48 * 4)\n",
    "    \"model\": {\n",
    "        \"input_dim\": 8,\n",
    "        \"model_dim\": 256,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_layers\": 4,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"max_epochs\": 3000,\n",
    "        \"base_lr\": 5e-4,        # Slightly lower max LR for stability\n",
    "        \"warmup_epochs\": 50,   # Warmup to prevent shock\n",
    "        \"clip_norm\": 2,\n",
    "    },\n",
    "    \"diffusion_steps\": 500,\n",
    "}\n",
    "\n",
    "MODEL_PATH = \"./best_gaussian_diffusion.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecee4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core: Sampling Function ---\n",
    "def sample_and_render(model, scheduler, device, num_samples=5, epoch=None):\n",
    "    \"\"\"Sample gaussians from the model and render them.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_dim = CONFIG[\"model\"][\"input_dim\"]\n",
    "\n",
    "        # Start from pure noise\n",
    "        seed = int(time.time() * 1000) % 1000000 if epoch is None else epoch * 1000\n",
    "        torch.manual_seed(seed)\n",
    "        x = torch.randn(num_samples, 1000, input_dim, device=device)\n",
    "        \n",
    "        # tqdm for sampling progress\n",
    "        iterator = tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", leave=False)\n",
    "        \n",
    "        for t in iterator:\n",
    "            timesteps = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "            predicted_noise = model(x, timesteps)\n",
    "            \n",
    "            # Scheduler constants\n",
    "            alpha_t = scheduler.alphas[t].to(device)\n",
    "            alpha_cumprod_t = scheduler.alphas_cumprod[t].to(device)\n",
    "            beta_t = scheduler.betas[t].to(device)\n",
    "            \n",
    "            # Denoising Step\n",
    "            coef1 = 1 / torch.sqrt(alpha_t)\n",
    "            coef2 = (1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)\n",
    "            \n",
    "            x = coef1 * (x - coef2 * predicted_noise)\n",
    "            x = torch.clamp(x, -3.5, 3.5) # Dynamic range clipping\n",
    "            \n",
    "            # Add noise (Langevin dynamics) except at last step\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                sigma_t = torch.sqrt(beta_t)\n",
    "                x += sigma_t * noise\n",
    "        \n",
    "        # Render\n",
    "        print(f\"\\n[Rendering] Saving {num_samples} samples...\")\n",
    "        for i in range(num_samples):\n",
    "            sample = x[i]\n",
    "            # Denormalize\n",
    "            xy, scale, rot, feat = denormalize_data(\n",
    "                sample[:, 0:2], sample[:, 2:4], sample[:, 4:5], sample[:, 5:8]\n",
    "            )\n",
    "            \n",
    "            # Ensure all tensors are float32 and contiguous\n",
    "            xy = xy.contiguous().float()\n",
    "            scale = scale.contiguous().float()\n",
    "            rot = rot.contiguous().float()\n",
    "            feat = feat.contiguous().float()\n",
    "            \n",
    "            # Ensure img_size is explicitly int tuple\n",
    "            img_size = (int(480), int(640))\n",
    "            epoch_suffix = f\"_epoch{epoch}\" if epoch is not None else \"\"\n",
    "            filename = f\"{CONFIG['output_dir']}/sample_{i}{epoch_suffix}\"\n",
    "            \n",
    "            render_and_save(xy, scale, rot, feat, filename, img_size)\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4638df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/leuven/366/vsc36675/txgs/.conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GaussianDiffusionTransformer(\n",
    "        input_dim=CONFIG[\"model\"][\"input_dim\"], \n",
    "        model_dim=CONFIG[\"model\"][\"model_dim\"], \n",
    "        n_heads=CONFIG[\"model\"][\"n_heads\"], \n",
    "        n_layers=CONFIG[\"model\"][\"n_layers\"]\n",
    ").to(device)\n",
    "\n",
    "scheduler = DiffusionScheduler(num_timesteps=CONFIG[\"diffusion_steps\"])\n",
    "\n",
    "# Load the trained model\n",
    "#model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "# Sample and render\n",
    "#sample_and_render(model, scheduler, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde58982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 819 files in ./data/small/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7093, 0.7096, 0.7099,  ..., 0.7679, 0.7681, 0.7684],\n",
       "         [0.7095, 0.7098, 0.7101,  ..., 0.7683, 0.7684, 0.7687],\n",
       "         [0.7097, 0.7100, 0.7103,  ..., 0.7686, 0.7688, 0.7691],\n",
       "         ...,\n",
       "         [0.5925, 0.5925, 0.5923,  ..., 0.3337, 0.6133, 0.8430],\n",
       "         [0.5925, 0.5925, 0.5921,  ..., 0.3387, 0.6110, 0.8099],\n",
       "         [0.5925, 0.5924, 0.5920,  ..., 0.3439, 0.6101, 0.8073]],\n",
       "\n",
       "        [[0.7180, 0.7183, 0.7186,  ..., 0.7839, 0.7841, 0.7844],\n",
       "         [0.7182, 0.7185, 0.7188,  ..., 0.7843, 0.7844, 0.7848],\n",
       "         [0.7185, 0.7188, 0.7191,  ..., 0.7846, 0.7848, 0.7851],\n",
       "         ...,\n",
       "         [0.5618, 0.5618, 0.5617,  ..., 0.3251, 0.6056, 0.8377],\n",
       "         [0.5618, 0.5618, 0.5616,  ..., 0.3306, 0.6038, 0.8051],\n",
       "         [0.5618, 0.5618, 0.5615,  ..., 0.3362, 0.6034, 0.8027]],\n",
       "\n",
       "        [[0.6956, 0.6960, 0.6964,  ..., 0.7804, 0.7805, 0.7808],\n",
       "         [0.6959, 0.6963, 0.6967,  ..., 0.7807, 0.7809, 0.7812],\n",
       "         [0.6962, 0.6967, 0.6971,  ..., 0.7811, 0.7813, 0.7816],\n",
       "         ...,\n",
       "         [0.4939, 0.4941, 0.4942,  ..., 0.3508, 0.6201, 0.8493],\n",
       "         [0.4940, 0.4941, 0.4942,  ..., 0.3554, 0.6179, 0.8168],\n",
       "         [0.4940, 0.4941, 0.4941,  ..., 0.3601, 0.6171, 0.8144]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the first batch to visualize\n",
    "import time\n",
    "import random\n",
    "torch.manual_seed(random.randint(0, 1000000))\n",
    "\n",
    "dataloader = create_dataloaders(CONFIG[\"data_dir\"], batch_size=CONFIG[\"batch_size\"])\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# Visualize the first sample in the batch\n",
    "sample = batch[0].to(device)  # First sample\n",
    "xy, scale, rot, feat = denormalize_data(\n",
    "    sample[:, 0:2], sample[:, 2:4], sample[:, 4:5], sample[:, 5:8]\n",
    ")\n",
    "\n",
    "xy = xy.contiguous().float()\n",
    "scale = scale.contiguous().float()\n",
    "rot = rot.contiguous().float()\n",
    "feat = feat.contiguous().float()\n",
    "\n",
    "render_and_save(xy, scale, rot, feat, f\"{CONFIG['output_dir']}/first_batch_sample\", (512, 512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
